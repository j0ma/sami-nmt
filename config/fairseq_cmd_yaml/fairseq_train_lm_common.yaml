arch: transformer_lm
dropout: 0.1
optimizer: adam
adam-betas: (0.9, 0.98)
weight-decay: 0.01
clip-norm: 0.0
lr: 0.0005
lr-scheduler: inverse_sqrt
warmup-updates: 4000
warmup-init-lr: 1e-07
max-tokens: 2048
update-freq: 16
#fp16: true
max-update: 50000
tokens-per-sample: 512
sample-break-mode: none
task: language_modeling
no-epoch-checkpoints: true
